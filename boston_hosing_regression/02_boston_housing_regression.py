# -*- coding: utf-8 -*-
"""02.boston_housing_regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tjpo5a7vpaPy6Fs-LqoaCzaVBip6Axay
"""

# 경고: 이 데이터셋에는 윤리적인 문제가 있습니다. 이 데이터셋의 저자들은 인종적 자기 분리가 주택 가격에 영향을 미친다고 가정할 수 있는 변수 "B"를 포함시켰습니다.
# 따라서 데이터 과학과 머신 러닝의 윤리적 문제를 설명하는 맥락이 아니라면, 우리는 이 데이터셋의 사용을 강력히 권장합니다.

#데이터 수집
import tensorflow as tf
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data(
    path='boston_housing.npz', test_split=0.2, seed=11317)
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

#2. 데이터의 유형이나 관련 필드 등을 분석 분류
  # CRIM     per capita crime rate by town
  # ZN       proportion of residential land zoned for lots over 25,000 sq.ft.
  # INDUS    proportion of non-retail business acres per town
  # CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
  # NOX      nitric oxides concentration (parts per 10 million)
  # RM       average number of rooms per dwelling
  # AGE      proportion of owner-occupied units built prior to 1940
  # DIS      weighted distances to five Boston employment centres
  # RAD      index of accessibility to radial highways
  # TAX      full-value property-tax rate per $10,000
  # PTRATIO  pupil-teacher ratio by town
  # B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
  # LSTAT    % lower status of the population
  # MEDV     Median value of owner-occupied homes in $1000's

  #CRIM 도시별 1인당 범죄율
  #ZN: 25,000제곱피트(약 2만 5천 제곱피트) 이상 면적의 주거용 토지 비율
  #INDUS: 도시별 비소매업용 에이커(약 1,000에이커) 비율
  #CHAS: 찰스 강 더미 변수(= 해당 구역이 강 경계에 있는 경우 1, 그렇지 않은 경우 0)
  #NOX: 일산화질소 농도(1,000만 분의 1)
  #RM: 가구당 평균 방 개수
  #AGE: 1940년 이전에 지어진 자가 주택 비율
  #DIS: 보스턴 5개 고용 센터까지의 가중 거리
  #RAD: 방사형 고속도로 접근성 지수
  #TAX: 1만 달러당 재산세 전액 세율
  #PTRATIO: 도시별 교사-학생 비율
  #B 1000(Bk - 0.63)^2, 여기서 Bk는 도시별 흑인 비율
  #LSTAT: 인구 하위 계층 비율(%)
  # 정답 :MEDV: 자가 주택의 중간 가격(1,000달러대)

#데이터 유형 검사
for ix,d in enumerate(x_train[0]):
  print(ix+1,"타입 : ",type(d))
print("정답데이터타입 :", type(y_train[0]))
data_label = ["범죄율","토지비율","비소매업면적","강인접","질소농도","평균방 수"
,"고택율","고용거리","고속접근","세율","교사-학생비율","흑인비율","하위계층비율"]

for ix,d in enumerate(x_train[0]):
  print(f"{ix}.{data_label[ix]}:{d}",end=" | ")
print()
print("평균주택가격:",y_train[0]*1000*1400,"원")

#
import matplotlib.pyplot as plt
for ix in range(len(x_train[0])):
  plt.subplot(4,4,ix+1)
  plt.scatter(y_train,x_train[:,ix],label =data_label[ix],s=1)
  plt.title(ix)
plt.show()

# 데이터 크기
for ix in range(len(x_train[0])):
  print(ix,".")
  print("max",x_train[:,ix].max(),end=":")
  print("min",x_train[:,ix].min(),end=":")
  print("std",x_train[:,ix].std(),end=":")
  print("mean",x_train[:,ix].mean(),end=":")
  print()

# 3번과 4번 필드를 제외한 데이터 정규화 스케일링
except_datas = [3,4]
norm_mean=[]
norm_std=[]
for ix in range(len(x_train[0])):
  if ix in except_datas:
    norm_mean.append(0)
    norm_std.append(0)
    continue
  norm_mean.append(x_train[:,ix].mean())
  norm_std.append(x_train[:,ix].std())
print(norm_mean)
print(norm_std)
def normal_data(target_data):
  for ix in range(len(target_data[0])):
    if ix in except_datas:
      continue
# 데이터를 표준 정규분포로 스케일링
    target_data[:,ix] = (target_data[:,ix]-norm_mean[ix])/norm_std[ix]
  return target_data

x_train = normal_data(x_train)

x_test = normal_data(x_test)
print(x_train[:3])
print(x_test[:3])
print(x_train[:,0].std())
print(x_train[:,0].mean())
print(x_train[:,-1].std())
print(x_train[:,-1].mean())
y_mean=y_train.mean()
y_std=y_train.std()
y_train = (y_train-y_mean)/y_std
y_test = (y_test-y_mean)/y_std

#4. 모델 생성
from tensorflow.keras import Input, Sequential
from tensorflow.keras.layers import Dense, Dropout
print(x_train.shape)

import numpy as np
import random
np.random.seed(11379)
tf.random.set_seed(11379)
random.seed(11379)
model = Sequential()
model.add(Input(shape=(13,)))
model.add(Dense(256,activation="relu"))
res = model(np.array([x_train[0]]))
print(len(res[0]))
print(res)
model.add(Dropout(0.4))
model.add(Dense(64,activation="relu"))
model.add(Dropout(0.3))
model.add(Dense(16,activation="relu"))
model.add(Dense(1))
model.compile(loss="mse",optimizer="SGD",metrics=['MAE'])
print(y_train.std())

#훈련시작
fhist = model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=500)

plt.plot(fhist.history["loss"],label="mse")
plt.plot(fhist.history["MAE"],label="mae")
plt.legend()
plt.show()

y_pred = model.predict(x_test)
plt.plot(y_test,y_test,label="y_True")
plt.scatter(y_pred,y_pred,label="y_Prue",c="red")
plt.legend()
plt.show()
print(y_test[1])
print(y_pred[1])

#오차율 계산
print(y_pred.shape)
print(y_test.shape)
y_pred = y_pred.reshape(-1)
print(y_pred.shape)

# (y - mean) / std > t*std+mean
#y_mean
#y_std
y_pred = y_pred*y_std+y_mean
y_pred = y_pred*y_std+y_mean
print(y_pred[1])
print(y_test[1])
print((1-(y_pred[1]/y_test[1]))*100)
rate = 1-y_pred/y_test
np.absolute(rate)
print(rate[:5])
err_rate_mean = rate[:5].mean()